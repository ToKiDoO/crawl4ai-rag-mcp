"""
Comprehensive unit tests for helper functions from crawl4ai_mcp.py and utils.py.

Test Coverage:
- crawl4ai_mcp.py helper functions: smart_chunk_markdown, extract_section_info, process_code_example,
  validate_script_path, validate_github_url, parse_sitemap, is_sitemap, is_txt, rerank_results,
  track_request, format_neo4j_error
- utils.py helper functions: create_embedding, create_embeddings_batch

Testing Approach:
- Comprehensive edge case coverage
- Proper mocking of external dependencies
- Parametrized tests for multiple scenarios
- Error handling validation
- Real-world usage patterns
"""

import os
import re
import sys
import time
from unittest.mock import MagicMock, Mock, mock_open, patch

import pytest

# Add src to path for imports
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", "src"))

# Import functions to test
from crawl4ai_mcp import (
    extract_section_info,
    format_neo4j_error,
    is_sitemap,
    is_txt,
    parse_sitemap,
    process_code_example,
    rerank_results,
    smart_chunk_markdown,
    track_request,
    validate_github_url,
    validate_script_path,
)

from utils import create_embedding, create_embeddings_batch


class TestSmartChunkMarkdown:
    """Test smart_chunk_markdown function for intelligent text chunking."""

    def test_simple_text_within_chunk_size(self):
        """Test text shorter than chunk size returns single chunk."""
        text = "This is a simple text that is shorter than the default chunk size."
        result = smart_chunk_markdown(text, chunk_size=1000)

        assert len(result) == 1
        assert result[0] == text

    def test_empty_text(self):
        """Test empty text returns empty list."""
        result = smart_chunk_markdown("", chunk_size=1000)
        assert result == []

    def test_whitespace_only_text(self):
        """Test whitespace-only text returns empty list after stripping."""
        result = smart_chunk_markdown("   \n\n  \t  ", chunk_size=1000)
        # Function strips whitespace, so empty string becomes empty list after filtering
        assert len(result) <= 1
        if result:
            assert result[0] == ""

    def test_text_exactly_chunk_size(self):
        """Test text exactly at chunk size."""
        text = "a" * 100
        result = smart_chunk_markdown(text, chunk_size=100)

        assert len(result) == 1
        assert result[0] == text

    def test_text_exceeds_chunk_size_no_breaks(self):
        """Test text exceeding chunk size with no natural breaks."""
        text = "a" * 200
        result = smart_chunk_markdown(text, chunk_size=100)

        assert len(result) == 2
        assert len(result[0]) == 100
        assert len(result[1]) == 100

    def test_code_block_boundary_chunking(self):
        """Test chunking respects code block boundaries."""
        text = (
            """This is some text before code.

```python
def function():
    return "hello"
```

This is text after the code block. """
            + "a" * 5000
        )

        result = smart_chunk_markdown(text, chunk_size=200)

        # Should break at code block boundary
        assert len(result) > 1
        # First chunk should end at or near code block
        assert "```" in result[0] or "```" in result[1]

    def test_paragraph_boundary_chunking(self):
        """Test chunking respects paragraph boundaries."""
        paragraph1 = "This is the first paragraph. " * 10
        paragraph2 = "This is the second paragraph. " * 10
        text = paragraph1 + "\n\n" + paragraph2

        result = smart_chunk_markdown(text, chunk_size=150)

        assert len(result) >= 1
        # Function may break at different boundaries based on 30% threshold
        # Just verify it produces reasonable chunks
        assert all(len(chunk) > 0 for chunk in result)

    def test_sentence_boundary_chunking(self):
        """Test chunking respects sentence boundaries."""
        text = "This is sentence one. This is sentence two. " * 20

        result = smart_chunk_markdown(text, chunk_size=100)

        assert len(result) > 1
        # Should prefer breaking at sentence boundaries when possible
        for chunk in result[:-1]:  # All but last chunk
            if ". " in chunk:
                assert chunk.strip().endswith(".")

    def test_minimum_chunk_size_threshold(self):
        """Test chunking respects 30% minimum threshold."""
        text = "Short start. " + "a" * 1000

        result = smart_chunk_markdown(text, chunk_size=100)

        # Should not break at very beginning due to 30% threshold
        assert len(result[0]) >= 30  # At least 30% of chunk_size

    def test_custom_chunk_size(self):
        """Test custom chunk sizes work correctly."""
        text = "a" * 1000

        result_50 = smart_chunk_markdown(text, chunk_size=50)
        result_200 = smart_chunk_markdown(text, chunk_size=200)

        assert len(result_50) > len(result_200)
        assert all(len(chunk) <= 50 for chunk in result_50)
        assert all(len(chunk) <= 200 for chunk in result_200)

    @pytest.mark.parametrize("chunk_size", [1, 10, 100, 1000, 10000])
    def test_various_chunk_sizes(self, chunk_size):
        """Test function works with various chunk sizes."""
        text = "This is a test text. " * 100

        result = smart_chunk_markdown(text, chunk_size=chunk_size)

        assert isinstance(result, list)
        assert all(isinstance(chunk, str) for chunk in result)
        if chunk_size < len(text):
            assert len(result) > 1

    def test_markdown_headers_preserved(self):
        """Test that markdown headers are preserved in chunks."""
        text = (
            """# Main Header

## Subsection

Content under subsection. """
            + "a" * 5000
        )

        result = smart_chunk_markdown(text, chunk_size=200)

        # Headers should be preserved
        headers_found = False
        for chunk in result:
            if "#" in chunk:
                headers_found = True
                break
        assert headers_found

    def test_real_world_markdown_document(self):
        """Test with realistic markdown document structure."""
        text = (
            """# Documentation

## Introduction
This is the introduction section with some content.

## Code Examples

```python
def example_function():
    return "Hello World"
```

### Subsection
More content here with explanations.

## Conclusion
Final thoughts and summary.
"""
            + "Additional content. " * 200
        )

        result = smart_chunk_markdown(text, chunk_size=300)

        assert len(result) >= 2
        assert all(chunk.strip() for chunk in result)  # No empty chunks


class TestExtractSectionInfo:
    """Test extract_section_info function for metadata extraction."""

    def test_no_headers(self):
        """Test chunk with no headers."""
        chunk = "This is just plain text with no headers whatsoever."

        result = extract_section_info(chunk)

        assert result["headers"] == ""
        assert result["char_count"] == len(chunk)
        assert result["word_count"] == len(chunk.split())

    def test_single_header(self):
        """Test chunk with single header."""
        chunk = "# Main Title\n\nSome content under the title."

        result = extract_section_info(chunk)

        assert result["headers"] == "# Main Title"
        assert result["char_count"] == len(chunk)
        assert result["word_count"] == len(chunk.split())

    def test_multiple_headers(self):
        """Test chunk with multiple headers."""
        chunk = """# Main Title

## Subsection

### Sub-subsection

Some content here."""

        result = extract_section_info(chunk)

        expected_headers = "# Main Title; ## Subsection; ### Sub-subsection"
        assert result["headers"] == expected_headers
        assert result["char_count"] == len(chunk)
        assert result["word_count"] == len(chunk.split())

    def test_headers_with_special_characters(self):
        """Test headers containing special characters."""
        chunk = """# Title with (Parentheses) and "Quotes"

## Section with `code` and *emphasis*

Content here."""

        result = extract_section_info(chunk)

        assert "Title with (Parentheses)" in result["headers"]
        assert "Section with `code`" in result["headers"]

    def test_empty_chunk(self):
        """Test empty chunk."""
        result = extract_section_info("")

        assert result["headers"] == ""
        assert result["char_count"] == 0
        assert result["word_count"] == 0

    def test_whitespace_only_chunk(self):
        """Test chunk with only whitespace."""
        chunk = "   \n\n  \t  "

        result = extract_section_info(chunk)

        assert result["headers"] == ""
        assert result["char_count"] == len(chunk)
        assert result["word_count"] == 0

    def test_headers_at_different_positions(self):
        """Test headers at beginning, middle, and end of chunk."""
        chunk = """# Start Header

Some content in the middle.

## Middle Header

More content.

### End Header"""

        result = extract_section_info(chunk)

        expected = "# Start Header; ## Middle Header; ### End Header"
        assert result["headers"] == expected

    @pytest.mark.parametrize(
        "header_level,symbol",
        [(1, "#"), (2, "##"), (3, "###"), (4, "####"), (5, "#####"), (6, "######")],
    )
    def test_various_header_levels(self, header_level, symbol):
        """Test various markdown header levels."""
        chunk = f"{symbol} Header Level {header_level}\n\nContent here."

        result = extract_section_info(chunk)

        assert result["headers"] == f"{symbol} Header Level {header_level}"

    def test_unicode_content(self):
        """Test with unicode content."""
        chunk = """# ÊµãËØïÊ†áÈ¢ò

## T√≠tulo en Espa√±ol

Content with √©mojis üöÄ and sp√©ci√°l characters."""

        result = extract_section_info(chunk)

        assert "ÊµãËØïÊ†áÈ¢ò" in result["headers"]
        assert "T√≠tulo en Espa√±ol" in result["headers"]
        assert result["char_count"] == len(chunk)

    def test_large_chunk_performance(self):
        """Test performance with large chunks."""
        # Create large chunk with multiple headers
        content = (
            "# Header 1\n"
            + "Content. " * 1000
            + "\n## Header 2\n"
            + "More content. " * 1000
        )

        start_time = time.time()
        result = extract_section_info(content)
        end_time = time.time()

        # Should complete within reasonable time
        assert end_time - start_time < 1.0
        assert "Header 1" in result["headers"]
        assert "Header 2" in result["headers"]


class TestProcessCodeExample:
    """Test process_code_example function for code processing."""

    @patch("crawl4ai_mcp.generate_code_example_summary")
    def test_process_code_example_success(self, mock_generate):
        """Test successful code example processing."""
        mock_generate.return_value = "Generated summary"

        args = ("def hello(): pass", "Context before", "Context after")
        result = process_code_example(args)

        assert result == "Generated summary"
        mock_generate.assert_called_once_with(
            "def hello(): pass",
            "Context before",
            "Context after",
        )

    @patch("crawl4ai_mcp.generate_code_example_summary")
    def test_process_code_example_with_none_context(self, mock_generate):
        """Test code example processing with None context."""
        mock_generate.return_value = "Summary with None context"

        args = ("print('hello')", None, None)
        result = process_code_example(args)

        assert result == "Summary with None context"
        mock_generate.assert_called_once_with("print('hello')", None, None)

    @patch("crawl4ai_mcp.generate_code_example_summary")
    def test_process_code_example_exception_propagation(self, mock_generate):
        """Test that exceptions from generate_code_example_summary are propagated."""
        mock_generate.side_effect = Exception("Generation failed")

        args = ("invalid code", "context", "context")

        with pytest.raises(Exception, match="Generation failed"):
            process_code_example(args)

    def test_process_code_example_tuple_unpacking(self):
        """Test that function correctly unpacks tuple arguments."""
        with patch("crawl4ai_mcp.generate_code_example_summary") as mock_generate:
            mock_generate.return_value = "Unpacked successfully"

            # Test with exactly 3 elements
            args = ("code", "before", "after")
            result = process_code_example(args)

            assert result == "Unpacked successfully"
            mock_generate.assert_called_once_with("code", "before", "after")

    @pytest.mark.parametrize(
        "code_sample",
        [
            "def function(): pass",
            "class MyClass: pass",
            "import os\nprint('hello')",
            "# Just a comment",
            "",
            "x = 1\ny = 2\nprint(x + y)",
        ],
    )
    def test_various_code_samples(self, code_sample):
        """Test processing various types of code samples."""
        with patch("crawl4ai_mcp.generate_code_example_summary") as mock_generate:
            mock_generate.return_value = f"Summary for: {code_sample[:20]}"

            args = (code_sample, "context", "context")
            result = process_code_example(args)

            assert result.startswith("Summary for:")
            mock_generate.assert_called_once_with(code_sample, "context", "context")


class TestValidateScriptPath:
    """Test validate_script_path function for script validation."""

    def test_none_input(self):
        """Test with None input."""
        result = validate_script_path(None)

        assert result["valid"] is False
        assert "Script path is required" in result["error"]

    def test_empty_string(self):
        """Test with empty string."""
        result = validate_script_path("")

        assert result["valid"] is False
        assert "Script path is required" in result["error"]

    def test_non_string_input(self):
        """Test with non-string input."""
        result = validate_script_path(123)

        assert result["valid"] is False
        assert "Script path is required" in result["error"]

    @patch("os.path.exists")
    def test_file_not_exists(self, mock_exists):
        """Test with non-existent file."""
        mock_exists.return_value = False

        result = validate_script_path("/nonexistent/script.py")

        assert result["valid"] is False
        assert "Script not found" in result["error"]
        mock_exists.assert_called_once_with("/nonexistent/script.py")

    def test_non_python_file(self):
        """Test with non-Python file extension."""
        with patch("os.path.exists", return_value=True):
            result = validate_script_path("/path/to/script.txt")

            assert result["valid"] is False
            assert "Only Python (.py) files are supported" in result["error"]

    @pytest.mark.parametrize("extension", [".js", ".txt", ".sh", ".java", ".cpp", ""])
    def test_various_non_python_extensions(self, extension):
        """Test various non-Python file extensions."""
        with patch("os.path.exists", return_value=True):
            result = validate_script_path(f"/path/to/script{extension}")

            assert result["valid"] is False
            assert "Only Python (.py) files are supported" in result["error"]

    @patch("builtins.open", new_callable=mock_open, read_data="print('hello')")
    @patch("os.path.exists")
    def test_valid_python_file(self, mock_exists, mock_file):
        """Test with valid Python file."""
        mock_exists.return_value = True

        result = validate_script_path("/path/to/valid_script.py")

        assert result["valid"] is True
        assert "error" not in result
        mock_exists.assert_called_once_with("/path/to/valid_script.py")
        mock_file.assert_called_once_with(
            "/path/to/valid_script.py",
            "r",
            encoding="utf-8",
        )

    @patch("builtins.open")
    @patch("os.path.exists")
    def test_file_read_permission_error(self, mock_exists, mock_open_func):
        """Test when file exists but cannot be read."""
        mock_exists.return_value = True
        mock_open_func.side_effect = PermissionError("Permission denied")

        result = validate_script_path("/path/to/restricted.py")

        assert result["valid"] is False
        assert "Cannot read script file" in result["error"]
        assert "Permission denied" in result["error"]

    @patch("builtins.open")
    @patch("os.path.exists")
    def test_file_read_unicode_error(self, mock_exists, mock_open_func):
        """Test when file has encoding issues."""
        mock_exists.return_value = True
        mock_open_func.side_effect = UnicodeDecodeError("utf-8", b"", 0, 1, "invalid")

        result = validate_script_path("/path/to/binary.py")

        assert result["valid"] is False
        assert "Cannot read script file" in result["error"]

    @pytest.mark.parametrize(
        "path",
        [
            "/absolute/path/to/script.py",
            "relative/path/to/script.py",
            "./local_script.py",
            "../parent_script.py",
            "script.py",
        ],
    )
    def test_various_path_formats(self, path):
        """Test various path formats."""
        with (
            patch("os.path.exists", return_value=True),
            patch("builtins.open", mock_open(read_data="# Python code")),
        ):
            result = validate_script_path(path)

            assert result["valid"] is True

    @patch("builtins.open", new_callable=mock_open, read_data="")
    @patch("os.path.exists")
    def test_empty_python_file(self, mock_exists, mock_file):
        """Test with empty Python file."""
        mock_exists.return_value = True

        result = validate_script_path("/path/to/empty.py")

        assert result["valid"] is True


class TestValidateGithubUrl:
    """Test validate_github_url function for GitHub URL validation."""

    def test_none_input(self):
        """Test with None input."""
        result = validate_github_url(None)

        assert result["valid"] is False
        assert "Repository URL is required" in result["error"]

    def test_empty_string(self):
        """Test with empty string."""
        result = validate_github_url("")

        assert result["valid"] is False
        assert "Repository URL is required" in result["error"]

    def test_non_string_input(self):
        """Test with non-string input."""
        result = validate_github_url(123)

        assert result["valid"] is False
        assert "Repository URL is required" in result["error"]

    def test_whitespace_only_url(self):
        """Test with whitespace-only URL."""
        result = validate_github_url("   \n\t   ")

        assert result["valid"] is False
        # After stripping, becomes empty string, so gets "required" error
        assert (
            "Repository URL is required" in result["error"]
            or "Please provide a valid GitHub repository URL" in result["error"]
        )

    def test_non_github_url(self):
        """Test with non-GitHub URL."""
        result = validate_github_url("https://gitlab.com/user/repo")

        assert result["valid"] is False
        assert "Please provide a valid GitHub repository URL" in result["error"]

    def test_url_without_protocol(self):
        """Test GitHub URL without protocol."""
        result = validate_github_url("github.com/user/repo")

        assert result["valid"] is False
        assert "Repository URL must start with https:// or git@" in result["error"]

    @pytest.mark.parametrize(
        "url,expected_name",
        [
            ("https://github.com/user/repo", "repo"),
            ("https://github.com/user/repo.git", "repo"),
            ("git@github.com:user/repo.git", "repo"),
            ("https://github.com/user/my-awesome-repo", "my-awesome-repo"),
            ("https://github.com/organization/project-name", "project-name"),
        ],
    )
    def test_valid_github_urls(self, url, expected_name):
        """Test various valid GitHub URL formats."""
        result = validate_github_url(url)

        assert result["valid"] is True
        assert result["repo_name"] == expected_name

    def test_https_url_with_trailing_slash(self):
        """Test HTTPS URL with trailing slash."""
        url = "https://github.com/user/repo/"
        result = validate_github_url(url)

        assert result["valid"] is True
        # Trailing slash may result in empty repo name from split
        assert "repo_name" in result

    def test_ssh_url_format(self):
        """Test SSH URL format."""
        url = "git@github.com:user/repo.git"
        result = validate_github_url(url)

        assert result["valid"] is True
        assert result["repo_name"] == "repo"

    def test_url_with_subdirectories(self):
        """Test URL with subdirectories (should still be valid)."""
        url = "https://github.com/user/repo/tree/main/subdir"
        result = validate_github_url(url)

        # URL contains github.com and starts with https, so should be valid
        assert result["valid"] is True

    @pytest.mark.parametrize(
        "invalid_url",
        [
            "ftp://github.com/user/repo",
            "http://github.com/user/repo",  # Not HTTPS
            "github.com/user/repo",  # No protocol
            "",
        ],
    )
    def test_invalid_url_formats(self, invalid_url):
        """Test various invalid URL formats."""
        result = validate_github_url(invalid_url)

        assert result["valid"] is False
        assert "error" in result

    def test_url_with_whitespace_gets_stripped(self):
        """Test that URLs with leading/trailing whitespace get stripped."""
        url = "  https://github.com/user/repo  "
        result = validate_github_url(url)

        assert result["valid"] is True
        assert result["repo_name"] == "repo"

    def test_case_insensitive_github_detection(self):
        """Test that GitHub detection is case insensitive."""
        url = "https://GITHUB.COM/user/repo"
        result = validate_github_url(url)

        assert result["valid"] is True
        assert result["repo_name"] == "repo"


class TestParseSitemap:
    """Test parse_sitemap function for XML sitemap parsing."""

    @patch("requests.get")
    def test_successful_sitemap_parsing(self, mock_get):
        """Test successful sitemap parsing."""
        sitemap_xml = """<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
    <url>
        <loc>https://example.com/page1</loc>
    </url>
    <url>
        <loc>https://example.com/page2</loc>
    </url>
</urlset>"""

        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.content = sitemap_xml.encode("utf-8")
        mock_get.return_value = mock_response

        result = parse_sitemap("https://example.com/sitemap.xml")

        assert len(result) == 2
        assert "https://example.com/page1" in result
        assert "https://example.com/page2" in result

    @patch("requests.get")
    def test_http_error_response(self, mock_get):
        """Test handling of HTTP error responses."""
        mock_response = Mock()
        mock_response.status_code = 404
        mock_get.return_value = mock_response

        result = parse_sitemap("https://example.com/nonexistent.xml")

        assert result == []

    @patch("requests.get")
    def test_malformed_xml(self, mock_get):
        """Test handling of malformed XML."""
        malformed_xml = """<?xml version="1.0"?>
<urlset>
    <url>
        <loc>https://example.com/page1</loc>
    </url>
    <!-- Missing closing tag
</urlset>"""

        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.content = malformed_xml.encode("utf-8")
        mock_get.return_value = mock_response

        with patch("crawl4ai_mcp.logger") as mock_logger:
            result = parse_sitemap("https://example.com/malformed.xml")

            assert result == []
            mock_logger.error.assert_called_once()

    @patch("requests.get")
    def test_empty_sitemap(self, mock_get):
        """Test parsing empty sitemap."""
        empty_xml = """<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
</urlset>"""

        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.content = empty_xml.encode("utf-8")
        mock_get.return_value = mock_response

        result = parse_sitemap("https://example.com/empty.xml")

        assert result == []

    @patch("requests.get")
    def test_sitemap_with_namespaces(self, mock_get):
        """Test sitemap with different XML namespaces."""
        namespaced_xml = """<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"
        xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
    <url>
        <loc>https://example.com/namespaced</loc>
    </url>
</urlset>"""

        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.content = namespaced_xml.encode("utf-8")
        mock_get.return_value = mock_response

        result = parse_sitemap("https://example.com/namespaced.xml")

        assert len(result) == 1
        assert "https://example.com/namespaced" in result

    @patch("requests.get")
    def test_sitemap_index_format(self, mock_get):
        """Test sitemap index format (contains <sitemap> instead of <url>)."""
        sitemap_index_xml = """<?xml version="1.0" encoding="UTF-8"?>
<sitemapindex xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
    <sitemap>
        <loc>https://example.com/sitemap1.xml</loc>
    </sitemap>
    <sitemap>
        <loc>https://example.com/sitemap2.xml</loc>
    </sitemap>
</sitemapindex>"""

        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.content = sitemap_index_xml.encode("utf-8")
        mock_get.return_value = mock_response

        result = parse_sitemap("https://example.com/sitemap_index.xml")

        # Should find <loc> elements regardless of parent element
        assert len(result) == 2
        assert "https://example.com/sitemap1.xml" in result
        assert "https://example.com/sitemap2.xml" in result

    @patch("requests.get")
    def test_network_error(self, mock_get):
        """Test handling of network errors."""
        mock_get.side_effect = Exception("Network error")

        # Function doesn't handle requests.get exceptions, so this will raise
        with pytest.raises(Exception, match="Network error"):
            parse_sitemap("https://example.com/sitemap.xml")

    @patch("requests.get")
    def test_large_sitemap(self, mock_get):
        """Test parsing large sitemap with many URLs."""
        # Generate sitemap with many URLs
        urls = [f"https://example.com/page{i}" for i in range(1000)]
        url_elements = "\n".join([f"<url><loc>{url}</loc></url>" for url in urls])
        large_xml = f"""<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
{url_elements}
</urlset>"""

        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.content = large_xml.encode("utf-8")
        mock_get.return_value = mock_response

        result = parse_sitemap("https://example.com/large.xml")

        assert len(result) == 1000
        assert "https://example.com/page1" in result
        assert "https://example.com/page999" in result

    @patch("requests.get")
    def test_unicode_urls_in_sitemap(self, mock_get):
        """Test sitemap with unicode URLs."""
        unicode_xml = """<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">
    <url>
        <loc>https://‰æã„Åà.„ÉÜ„Çπ„Éà/È°µÈù¢</loc>
    </url>
    <url>
        <loc>https://example.com/p√°gina-espa√±ol</loc>
    </url>
</urlset>"""

        mock_response = Mock()
        mock_response.status_code = 200
        mock_response.content = unicode_xml.encode("utf-8")
        mock_get.return_value = mock_response

        result = parse_sitemap("https://example.com/unicode.xml")

        assert len(result) == 2
        assert any("‰æã„Åà.„ÉÜ„Çπ„Éà" in url for url in result)
        assert any("p√°gina-espa√±ol" in url for url in result)


class TestIsSitemap:
    """Test is_sitemap function for sitemap URL detection."""

    @pytest.mark.parametrize(
        "url,expected",
        [
            ("https://example.com/sitemap.xml", True),
            ("http://test.com/sitemap.xml", True),
            ("https://site.org/sitemaps/sitemap.xml", True),
            ("https://example.com/data/sitemap.xml", True),
            ("https://example.com/sitemap", True),  # Contains 'sitemap' in path
            (
                "https://example.com/sitemaps/index.html",
                True,
            ),  # Contains 'sitemap' in path
            ("https://example.com/page.html", False),
            ("https://example.com/data.xml", False),
            ("https://example.com/", False),
            ("https://example.com/map.xml", False),  # Contains 'map' but not 'sitemap'
            ("https://example.com/site-map.xml", False),  # Hyphenated doesn't count
        ],
    )
    def test_sitemap_detection(self, url, expected):
        """Test sitemap detection with various URL patterns."""
        result = is_sitemap(url)
        assert result == expected

    def test_case_sensitivity(self):
        """Test that sitemap detection is case sensitive."""
        # The function checks for exact 'sitemap' string
        assert is_sitemap("https://example.com/SITEMAP.xml") is False
        assert is_sitemap("https://example.com/Sitemap.xml") is False
        assert is_sitemap("https://example.com/sitemap.xml") is True

    def test_path_vs_domain(self):
        """Test that function checks path, not domain."""
        assert is_sitemap("https://sitemap.com/page.html") is False
        assert is_sitemap("https://example.com/sitemap") is True

    def test_url_fragments_and_queries(self):
        """Test URLs with fragments and query parameters."""
        assert is_sitemap("https://example.com/sitemap.xml?v=1") is True
        assert is_sitemap("https://example.com/sitemap.xml#section") is True
        assert is_sitemap("https://example.com/page.html?sitemap=true") is False

    def test_empty_and_invalid_urls(self):
        """Test with empty and invalid URLs."""
        assert is_sitemap("") is False
        assert is_sitemap("not-a-url") is False
        assert is_sitemap("https://") is False


class TestIsTxt:
    """Test is_txt function for text file URL detection."""

    @pytest.mark.parametrize(
        "url,expected",
        [
            ("https://example.com/file.txt", True),
            ("http://test.com/data.txt", True),
            ("https://site.org/readme.txt", True),
            ("https://example.com/FILE.TXT", False),  # Case sensitive
            ("https://example.com/file.text", False),  # Wrong extension
            ("https://example.com/file.txt.backup", False),  # Extension not at end
            ("https://example.com/txtfile", False),  # No extension
            ("https://example.com/file.html", False),
            ("https://example.com/", False),
            (
                "https://example.com/file.txt?param=value",
                False,
            ),  # Query params break endswith
            ("https://example.com/file.txt#section", False),  # Fragment breaks endswith
        ],
    )
    def test_txt_detection(self, url, expected):
        """Test text file detection with various URL patterns."""
        result = is_txt(url)
        assert result == expected

    def test_case_sensitivity(self):
        """Test that txt detection is case sensitive."""
        assert is_txt("https://example.com/file.txt") is True
        assert is_txt("https://example.com/file.TXT") is False
        assert is_txt("https://example.com/file.Txt") is False

    def test_multiple_extensions(self):
        """Test URLs with multiple extensions."""
        assert is_txt("https://example.com/archive.tar.txt") is True
        assert is_txt("https://example.com/file.txt.gz") is False

    def test_empty_and_invalid_urls(self):
        """Test with empty and invalid URLs."""
        assert is_txt("") is False
        assert is_txt("not-a-url") is False
        assert is_txt("https://") is False

    def test_path_with_txt_in_middle(self):
        """Test paths that contain 'txt' but don't end with .txt."""
        assert is_txt("https://example.com/txt/file.html") is False
        assert is_txt("https://example.com/textbook/page.html") is False


class TestRerankResults:
    """Test rerank_results function for result reranking."""

    def test_empty_results(self):
        """Test reranking with empty results."""
        mock_model = MagicMock()
        result = rerank_results(mock_model, "query", [])

        assert result == []
        mock_model.predict.assert_not_called()

    def test_none_model(self):
        """Test reranking with None model."""
        results = [{"content": "test"}]
        result = rerank_results(None, "query", results)

        assert result == results

    def test_single_result_reranking(self):
        """Test reranking single result."""
        mock_model = MagicMock()
        mock_model.predict.return_value = [0.8]

        results = [{"content": "relevant content", "id": "1"}]
        result = rerank_results(mock_model, "test query", results)

        assert len(result) == 1
        assert result[0]["rerank_score"] == 0.8
        assert result[0]["content"] == "relevant content"
        mock_model.predict.assert_called_once_with([["test query", "relevant content"]])

    def test_multiple_results_reranking(self):
        """Test reranking multiple results."""
        mock_model = MagicMock()
        mock_model.predict.return_value = [0.9, 0.3, 0.7]

        results = [
            {"content": "highly relevant", "id": "1"},
            {"content": "not relevant", "id": "2"},
            {"content": "somewhat relevant", "id": "3"},
        ]

        result = rerank_results(mock_model, "test query", results)

        # Should be sorted by rerank_score (descending)
        assert len(result) == 3
        assert result[0]["rerank_score"] == 0.9  # Highest score first
        assert result[1]["rerank_score"] == 0.7
        assert result[2]["rerank_score"] == 0.3  # Lowest score last
        assert result[0]["id"] == "1"

    def test_custom_content_key(self):
        """Test reranking with custom content key."""
        mock_model = MagicMock()
        mock_model.predict.return_value = [0.5]

        results = [{"text": "content here", "id": "1"}]
        result = rerank_results(mock_model, "query", results, content_key="text")

        mock_model.predict.assert_called_once_with([["query", "content here"]])
        assert result[0]["rerank_score"] == 0.5

    def test_missing_content_key(self):
        """Test reranking when content key is missing."""
        mock_model = MagicMock()
        mock_model.predict.return_value = [0.1]

        results = [{"id": "1", "title": "no content field"}]
        result = rerank_results(mock_model, "query", results)

        # Should use empty string for missing content
        mock_model.predict.assert_called_once_with([["query", ""]])

    def test_reranking_exception_handling(self):
        """Test handling of exceptions during reranking."""
        mock_model = MagicMock()
        mock_model.predict.side_effect = Exception("Model error")

        results = [{"content": "test", "id": "1"}]

        with patch("crawl4ai_mcp.logger") as mock_logger:
            result = rerank_results(mock_model, "query", results)

            # Should return original results on error
            assert result == results
            mock_logger.error.assert_called_once()

    def test_score_conversion_to_float(self):
        """Test that scores are converted to float."""
        mock_model = MagicMock()
        # Return numpy-like array or other numeric type
        mock_model.predict.return_value = [0.75]

        results = [{"content": "test"}]
        result = rerank_results(mock_model, "query", results)

        assert isinstance(result[0]["rerank_score"], float)
        assert result[0]["rerank_score"] == 0.75

    def test_preserve_original_fields(self):
        """Test that original result fields are preserved."""
        mock_model = MagicMock()
        mock_model.predict.return_value = [0.6]

        results = [
            {"content": "test", "url": "http://test.com", "metadata": {"key": "value"}},
        ]
        result = rerank_results(mock_model, "query", results)

        assert result[0]["content"] == "test"
        assert result[0]["url"] == "http://test.com"
        assert result[0]["metadata"] == {"key": "value"}
        assert result[0]["rerank_score"] == 0.6

    @pytest.mark.parametrize(
        "scores,expected_order",
        [
            ([0.1, 0.9, 0.5], [1, 2, 0]),  # Indices in descending score order
            ([0.5, 0.5, 0.5], [0, 1, 2]),  # Equal scores maintain order
            ([0.9, 0.8, 0.7, 0.6], [0, 1, 2, 3]),  # Already sorted
            ([0.1, 0.2, 0.3, 0.4], [3, 2, 1, 0]),  # Reverse order
        ],
    )
    def test_various_scoring_patterns(self, scores, expected_order):
        """Test reranking with various scoring patterns."""
        mock_model = MagicMock()
        mock_model.predict.return_value = scores

        results = [{"content": f"content{i}", "id": i} for i in range(len(scores))]
        result = rerank_results(mock_model, "query", results)

        for i, expected_idx in enumerate(expected_order):
            assert result[i]["id"] == expected_idx


class TestTrackRequest:
    """Test track_request decorator for request tracking."""

    def test_track_request_decorator_success(self):
        """Test track_request decorator on successful function."""
        with patch("crawl4ai_mcp.logger") as mock_logger:

            @track_request("test_tool")
            async def test_function(ctx):
                return "success"

            # Create mock context
            mock_ctx = MagicMock()

            # Run the decorated function
            import asyncio

            result = asyncio.run(test_function(mock_ctx))

            assert result == "success"

            # Check logging calls
            assert mock_logger.info.call_count >= 2  # Start and completion
            assert mock_logger.debug.call_count >= 1  # Arguments logging

            # Check log messages
            log_calls = [call.args[0] for call in mock_logger.info.call_args_list]
            assert any("Starting test_tool request" in msg for msg in log_calls)
            assert any("Completed test_tool" in msg for msg in log_calls)

    def test_track_request_decorator_exception(self):
        """Test track_request decorator on function that raises exception."""
        with patch("crawl4ai_mcp.logger") as mock_logger:

            @track_request("failing_tool")
            async def failing_function(ctx):
                raise ValueError("Test error")

            mock_ctx = MagicMock()

            import asyncio

            with pytest.raises(ValueError, match="Test error"):
                asyncio.run(failing_function(mock_ctx))

            # Check error logging
            assert mock_logger.error.call_count >= 1
            assert mock_logger.debug.call_count >= 1  # Traceback logging

            error_calls = [call.args[0] for call in mock_logger.error.call_args_list]
            assert any("Failed failing_tool" in msg for msg in error_calls)

    def test_track_request_preserves_function_metadata(self):
        """Test that decorator preserves original function metadata."""

        @track_request("test_tool")
        async def original_function(ctx, param1, param2="default"):
            """Original docstring."""
            return f"{param1}-{param2}"

        # Check that functools.wraps preserved metadata
        assert original_function.__name__ == "original_function"
        assert "Original docstring" in original_function.__doc__

    def test_track_request_with_args_and_kwargs(self):
        """Test track_request with function arguments and keyword arguments."""
        with patch("crawl4ai_mcp.logger") as mock_logger:

            @track_request("parameterized_tool")
            async def parameterized_function(ctx, arg1, arg2, kwarg1="default"):
                return f"{arg1}-{arg2}-{kwarg1}"

            mock_ctx = MagicMock()

            import asyncio

            result = asyncio.run(parameterized_function(mock_ctx, "a", "b", kwarg1="c"))

            assert result == "a-b-c"

            # Check that arguments were logged
            debug_calls = [call.args[0] for call in mock_logger.debug.call_args_list]
            assert any("Arguments:" in msg for msg in debug_calls)

    def test_track_request_timing_measurement(self):
        """Test that track_request measures execution time."""
        with patch("crawl4ai_mcp.logger") as mock_logger:

            @track_request("timed_tool")
            async def slow_function(ctx):
                import asyncio

                await asyncio.sleep(0.1)  # 100ms delay
                return "done"

            mock_ctx = MagicMock()

            import asyncio

            result = asyncio.run(slow_function(mock_ctx))

            assert result == "done"

            # Check that timing was logged
            log_calls = [call.args[0] for call in mock_logger.info.call_args_list]
            completion_msgs = [
                msg for msg in log_calls if "Completed timed_tool" in msg
            ]
            assert len(completion_msgs) >= 1

            # Should contain timing information
            timing_msg = completion_msgs[0]
            assert "in " in timing_msg and "s" in timing_msg

    def test_track_request_unique_request_ids(self):
        """Test that each request gets a unique ID."""
        request_ids = []

        def capture_request_id(*args, **kwargs):
            msg = args[0]
            # Extract request ID from log message [ID] format

            match = re.search(r"\[([a-f0-9]{8})\]", msg)
            if match:
                request_ids.append(match.group(1))

        with patch("crawl4ai_mcp.logger") as mock_logger:
            mock_logger.info.side_effect = capture_request_id

            @track_request("id_test_tool")
            async def test_function(ctx):
                return "success"

            mock_ctx = MagicMock()

            import asyncio

            # Run function multiple times
            for _ in range(3):
                asyncio.run(test_function(mock_ctx))

            # Each execution should have unique request ID
            unique_ids = set(request_ids)
            assert len(unique_ids) >= 3  # At least 3 unique IDs


class TestFormatNeo4jError:
    """Test format_neo4j_error function for error message formatting."""

    def test_authentication_error(self):
        """Test formatting of authentication errors."""
        auth_error = Exception("Authentication failed - invalid credentials")
        result = format_neo4j_error(auth_error)

        assert "Neo4j authentication failed" in result
        assert "NEO4J_USERNAME and NEO4J_PASSWORD" in result

    def test_unauthorized_error(self):
        """Test formatting of unauthorized errors."""
        unauth_error = Exception("Unauthorized access to database")
        result = format_neo4j_error(unauth_error)

        assert "Neo4j authentication failed" in result
        assert "NEO4J_USERNAME and NEO4J_PASSWORD" in result

    def test_connection_refused_error(self):
        """Test formatting of connection refused errors."""
        conn_error = Exception("Connection refused by server")
        result = format_neo4j_error(conn_error)

        assert "Cannot connect to Neo4j" in result
        assert "NEO4J_URI" in result
        assert "ensure Neo4j is running" in result

    def test_connection_timeout_error(self):
        """Test formatting of connection timeout errors."""
        timeout_error = Exception("Connection timeout after 30 seconds")
        result = format_neo4j_error(timeout_error)

        assert "Cannot connect to Neo4j" in result
        assert "NEO4J_URI" in result

    def test_database_error(self):
        """Test formatting of database-specific errors."""
        db_error = Exception("Database 'nonexistent' does not exist")
        result = format_neo4j_error(db_error)

        assert "Neo4j database error" in result
        assert "database exists and is accessible" in result

    def test_generic_error(self):
        """Test formatting of generic/unknown errors."""
        generic_error = Exception("Some unknown Neo4j error occurred")
        result = format_neo4j_error(generic_error)

        assert "Neo4j error:" in result
        assert "Some unknown Neo4j error occurred" in result

    @pytest.mark.parametrize(
        "error_msg,expected_check",
        [
            ("AUTHENTICATION failed", "authentication failed"),
            ("Connection REFUSED", "Cannot connect to Neo4j"),
            ("DATABASE not found", "database error"),
            ("UNAUTHORIZED access", "authentication failed"),
            ("Connection TIMEOUT", "Cannot connect to Neo4j"),
            ("Random error message", "Neo4j error:"),
        ],
    )
    def test_case_insensitive_error_detection(self, error_msg, expected_check):
        """Test that error detection is case insensitive."""
        error = Exception(error_msg)
        result = format_neo4j_error(error)

        assert expected_check.lower() in result.lower()

    def test_multiple_error_keywords(self):
        """Test error with multiple keywords (should match first applicable)."""
        multi_error = Exception("Authentication failed - connection refused")
        result = format_neo4j_error(multi_error)

        # Should match authentication first (as it appears first in the conditions)
        assert "authentication failed" in result

    def test_error_with_empty_message(self):
        """Test error with empty message."""
        empty_error = Exception("")
        result = format_neo4j_error(empty_error)

        assert "Neo4j error:" in result

    def test_non_string_error_conversion(self):
        """Test that non-string errors are converted properly."""

        class CustomError(Exception):
            def __str__(self):
                return "custom error message"

        custom_error = CustomError()
        result = format_neo4j_error(custom_error)

        assert "custom error message" in result


class TestCreateEmbedding:
    """Test create_embedding function for single text embedding."""

    @patch("utils.create_embeddings_batch")
    def test_create_embedding_success(self, mock_batch):
        """Test successful single embedding creation."""
        mock_batch.return_value = [[0.1, 0.2, 0.3]]

        result = create_embedding("test text")

        assert result == [0.1, 0.2, 0.3]
        mock_batch.assert_called_once_with(["test text"])

    @patch("utils.create_embeddings_batch")
    def test_create_embedding_empty_result(self, mock_batch):
        """Test handling of empty batch result."""
        mock_batch.return_value = []

        result = create_embedding("test text")

        # Should return default embedding of zeros
        assert len(result) == 1536
        assert all(val == 0.0 for val in result)

    @patch("utils.create_embeddings_batch")
    def test_create_embedding_exception_handling(self, mock_batch):
        """Test exception handling returns default embedding."""
        mock_batch.side_effect = Exception("API error")

        with patch("sys.stderr") as mock_stderr:
            result = create_embedding("test text")

            assert len(result) == 1536
            assert all(val == 0.0 for val in result)

    def test_create_embedding_empty_text(self):
        """Test create_embedding with empty text."""
        with patch("utils.create_embeddings_batch") as mock_batch:
            mock_batch.return_value = [[0.0] * 1536]

            result = create_embedding("")

            assert len(result) == 1536
            mock_batch.assert_called_once_with([""])

    def test_create_embedding_unicode_text(self):
        """Test create_embedding with unicode text."""
        with patch("utils.create_embeddings_batch") as mock_batch:
            mock_batch.return_value = [[0.5] * 1536]

            unicode_text = "ÊµãËØïÊñáÊú¨ with √©mojis üöÄ"
            result = create_embedding(unicode_text)

            assert len(result) == 1536
            mock_batch.assert_called_once_with([unicode_text])

    @pytest.mark.parametrize(
        "text",
        [
            "Simple text",
            "Text with numbers 123",
            "Text with special chars !@#$%",
            "Very long text " * 100,
            "\n\tWhitespace\n\t",
        ],
    )
    def test_various_text_inputs(self, text):
        """Test create_embedding with various text inputs."""
        with patch("utils.create_embeddings_batch") as mock_batch:
            mock_batch.return_value = [[0.1] * 1536]

            result = create_embedding(text)

            assert len(result) == 1536
            mock_batch.assert_called_once_with([text])


class TestCreateEmbeddingsBatch:
    """Test create_embeddings_batch function for batch embedding creation."""

    @patch("openai.embeddings.create")
    def test_successful_batch_creation(self, mock_openai):
        """Test successful batch embedding creation."""
        # Mock OpenAI response
        mock_response = MagicMock()
        mock_response.data = [
            MagicMock(embedding=[0.1] * 1536),
            MagicMock(embedding=[0.2] * 1536),
        ]
        mock_openai.return_value = mock_response

        texts = ["text 1", "text 2"]
        result = create_embeddings_batch(texts)

        assert len(result) == 2
        assert result[0] == [0.1] * 1536
        assert result[1] == [0.2] * 1536
        mock_openai.assert_called_once_with(model="text-embedding-3-small", input=texts)

    def test_empty_input(self):
        """Test with empty input list."""
        result = create_embeddings_batch([])
        assert result == []

    @patch("openai.embeddings.create")
    @patch("time.sleep")  # Mock sleep to speed up test
    def test_retry_mechanism(self, mock_sleep, mock_openai):
        """Test retry mechanism on API failures."""
        # First two calls fail, third succeeds
        mock_openai.side_effect = [
            Exception("Rate limit"),
            Exception("Server error"),
            MagicMock(data=[MagicMock(embedding=[0.1] * 1536)]),
        ]

        with patch("sys.stderr"):  # Suppress error output
            result = create_embeddings_batch(["test"])

        assert len(result) == 1
        assert result[0] == [0.1] * 1536
        assert mock_openai.call_count == 3
        assert mock_sleep.call_count == 2  # Two retries

    @patch("openai.embeddings.create")
    @patch("time.sleep")
    def test_max_retries_exceeded_fallback(self, mock_sleep, mock_openai):
        """Test fallback to individual requests when batch fails."""
        # Batch calls always fail
        mock_openai.side_effect = Exception("Persistent error")

        texts = ["text1", "text2"]

        with patch("sys.stderr"):  # Suppress error output
            result = create_embeddings_batch(texts)

        # Should return zero embeddings for failed texts
        assert len(result) == 2
        assert all(emb == [0.0] * 1536 for emb in result)

    @patch("openai.embeddings.create")
    @patch("time.sleep")
    def test_individual_fallback_partial_success(self, mock_sleep, mock_openai):
        """Test individual fallback with partial success."""

        # Batch call fails, individual calls have mixed success
        def side_effect_func(*args, **kwargs):
            if isinstance(kwargs.get("input"), list) and len(kwargs["input"]) > 1:
                # Batch call fails
                raise Exception("Batch failed")
            # Individual calls - first succeeds, second fails
            if kwargs["input"] == ["text1"]:
                return MagicMock(data=[MagicMock(embedding=[0.1] * 1536)])
            raise Exception("Individual failed")

        mock_openai.side_effect = side_effect_func

        texts = ["text1", "text2"]

        with patch("sys.stderr"):  # Suppress error output
            result = create_embeddings_batch(texts)

        assert len(result) == 2
        assert result[0] == [0.1] * 1536  # First succeeded
        assert result[1] == [0.0] * 1536  # Second failed, fallback embedding

    @patch("openai.embeddings.create")
    def test_exponential_backoff(self, mock_openai):
        """Test exponential backoff in retry delays."""
        mock_openai.side_effect = [
            Exception("Error 1"),
            Exception("Error 2"),
            MagicMock(data=[MagicMock(embedding=[0.1] * 1536)]),
        ]

        with patch("time.sleep") as mock_sleep, patch("sys.stderr"):
            result = create_embeddings_batch(["test"])

            # Check exponential backoff: 1s, then 2s
            sleep_calls = [call.args[0] for call in mock_sleep.call_args_list]
            assert sleep_calls == [1.0, 2.0]

    @pytest.mark.parametrize("batch_size", [1, 5, 10, 100])
    def test_various_batch_sizes(self, batch_size):
        """Test with various batch sizes."""
        with patch("openai.embeddings.create") as mock_openai:
            mock_response = MagicMock()
            mock_response.data = [
                MagicMock(embedding=[0.1] * 1536) for _ in range(batch_size)
            ]
            mock_openai.return_value = mock_response

            texts = [f"text {i}" for i in range(batch_size)]
            result = create_embeddings_batch(texts)

            assert len(result) == batch_size
            assert all(len(emb) == 1536 for emb in result)

    def test_unicode_texts(self):
        """Test batch creation with unicode texts."""
        with patch("openai.embeddings.create") as mock_openai:
            mock_response = MagicMock()
            mock_response.data = [
                MagicMock(embedding=[0.1] * 1536),
                MagicMock(embedding=[0.2] * 1536),
            ]
            mock_openai.return_value = mock_response

            texts = ["English text", "ÊµãËØï‰∏≠ÊñáÊñáÊú¨"]
            result = create_embeddings_batch(texts)

            assert len(result) == 2
            mock_openai.assert_called_once_with(
                model="text-embedding-3-small",
                input=texts,
            )


# Update task progress
def update_todo_progress():
    """Update TODO progress after creating tests."""
    print("‚úÖ Created comprehensive unit tests for helper functions")
    print("üìä Test Coverage:")
    print("   - crawl4ai_mcp.py: 11 helper functions tested")
    print("   - utils.py: 2 helper functions tested")
    print("   - Total test methods: 90+ comprehensive tests")
    print("   - Edge cases, error handling, and parametrized tests included")


# Run progress update when module is imported during testing
if __name__ == "__main__":
    update_todo_progress()
