name: CI/CD Pipeline - Test & Coverage

permissions:
  contents: read
  pull-requests: write
  issues: write
  checks: write
  statuses: write

on:
  push:
    branches: [ main, develop, fix/*, feature/* ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:  # Allow manual triggers

# Ensure only one workflow runs at a time per PR/branch
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  # Global environment variables
  PYTHON_VERSION: "3.12"
  UV_CACHE_DIR: ~/.cache/uv
  COVERAGE_THRESHOLD: 80

jobs:
  # ========================================
  # LINTING AND CODE QUALITY
  # ========================================
  lint:
    name: Code Quality & Linting
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install UV
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          echo "$HOME/.cargo/bin" >> $GITHUB_PATH

      - name: Cache UV dependencies
        uses: actions/cache@v4
        with:
          path: ${{ env.UV_CACHE_DIR }}
          key: uv-${{ runner.os }}-${{ env.PYTHON_VERSION }}-${{ hashFiles('**/uv.lock') }}
          restore-keys: |
            uv-${{ runner.os }}-${{ env.PYTHON_VERSION }}-

      - name: Install dependencies
        run: uv sync --frozen

      - name: Run ruff linting
        run: |
          echo "ðŸ” Running code linting..."
          uv run ruff check src/ tests/ --output-format=github

      - name: Run ruff formatting check
        run: |
          echo "ðŸŽ¨ Checking code formatting..."
          uv run ruff format src/ tests/ --check

      - name: Run type checking (if mypy available)
        run: |
          echo "ðŸ”¬ Running type checking..."
          if uv run python -c "import mypy" 2>/dev/null; then
            uv run mypy src/ || echo "Type checking failed but continuing..."
          else
            echo "MyPy not available, skipping type checking"
          fi
        continue-on-error: true

  # ========================================
  # UNIT TESTS
  # ========================================
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: lint
    
    strategy:
      fail-fast: false
      matrix:
        python-version: ["3.12", "3.13"]
        test-group: ["core", "adapters", "interfaces"]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install UV
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          echo "$HOME/.cargo/bin" >> $GITHUB_PATH

      - name: Cache UV dependencies
        uses: actions/cache@v4
        with:
          path: ${{ env.UV_CACHE_DIR }}
          key: uv-${{ runner.os }}-${{ matrix.python-version }}-${{ hashFiles('**/uv.lock') }}
          restore-keys: |
            uv-${{ runner.os }}-${{ matrix.python-version }}-

      - name: Install dependencies
        run: uv sync --frozen

      - name: Run tests for ${{ matrix.test-group }}
        env:
          # Core test environment configuration
          OPENAI_API_KEY: test-key-for-mocks
          VECTOR_DATABASE: qdrant
          DATABASE_PROVIDER: qdrant
          QDRANT_URL: http://localhost:6333
          QDRANT_API_KEY: ""
          QDRANT_COLLECTION_NAME: test_crawled_pages
          QDRANT_EMBEDDING_MODEL: text-embedding-3-small
          SEARXNG_URL: http://localhost:8081
          SEARXNG_TEST_URL: http://localhost:8081
          USE_RERANKING: "false"
          RERANKER_MODEL: BAAI/bge-reranker-v2-m3
          CHUNK_SIZE: "2000"
          CHUNK_OVERLAP: "200"
          TEST_TIMEOUT: "30"
          TEST_PARALLEL_WORKERS: "4"
          PYTEST_VERBOSE: "true"
          PYTHONPATH: ${{ github.workspace }}/src
          TESTING: "true"
          CI: "true"
        run: |
          case "${{ matrix.test-group }}" in
            "core")
              echo "ðŸ§ª Running core tests..."
              uv run pytest tests/test_utils_refactored.py tests/test_database_factory.py tests/test_crawl4ai_mcp.py \
                -v --tb=short --cov=src --cov-report=xml --cov-report=term-missing -m "not integration" \
                --perf-monitor --perf-output=performance_metrics_${{ matrix.test-group }}_py${{ matrix.python-version }}.json
              ;;
            "adapters")
              echo "ðŸ”— Running adapter tests..."
              uv run pytest tests/test_supabase_adapter.py tests/test_qdrant_adapter.py \
                -v --tb=short --cov=src --cov-report=xml --cov-report=term-missing -m "not integration" \
                --perf-monitor --perf-output=performance_metrics_${{ matrix.test-group }}_py${{ matrix.python-version }}.json
              ;;
            "interfaces")
              echo "ðŸ“‹ Running interface tests..."
              uv run pytest tests/test_database_interface.py tests/test_integration_simple.py \
                -v --tb=short --cov=src --cov-report=xml --cov-report=term-missing -m "not integration" \
                --perf-monitor --perf-output=performance_metrics_${{ matrix.test-group }}_py${{ matrix.python-version }}.json
              ;;
          esac

      - name: Upload coverage artifacts for ${{ matrix.test-group }}
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: coverage-${{ matrix.test-group }}-py${{ matrix.python-version }}
          path: |
            coverage.xml
            coverage.json
            .coverage
          retention-days: 7
      
      - name: Upload performance metrics
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-metrics-${{ matrix.test-group }}-py${{ matrix.python-version }}
          path: performance_metrics_${{ matrix.test-group }}_py${{ matrix.python-version }}.json
          retention-days: 30

  # ========================================
  # INTEGRATION TESTS WITH DOCKER SERVICES
  # ========================================
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: unit-tests
    
    services:
      # We'll use docker-compose instead of services for better control
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install UV
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          echo "$HOME/.cargo/bin" >> $GITHUB_PATH

      - name: Cache UV dependencies
        uses: actions/cache@v4
        with:
          path: ${{ env.UV_CACHE_DIR }}
          key: uv-${{ runner.os }}-${{ env.PYTHON_VERSION }}-${{ hashFiles('**/uv.lock') }}

      - name: Install dependencies
        run: uv sync --frozen

      - name: Set up test environment
        run: |
          mkdir -p searxng-test
          echo "Creating minimal SearXNG test config..."

      - name: Create SearXNG test configuration
        run: |
          mkdir -p searxng-test
          cat > searxng-test/settings.yml << 'EOF'
          general:
            secret_key: "test-secret-key-for-ci-cd-environment"
            debug: false
            instance_name: "SearXNG Test"

          server:
            port: 8080
            bind_address: "0.0.0.0"
            secret_key: "test-secret-key-for-ci-cd-environment"
            base_url: "http://localhost:8081/"

          redis:
            url: "redis://valkey-test:6379/0"

          search:
            safe_search: 0
            autocomplete: ""
            default_lang: "en"
            formats:
              - html
              - json

          engines:
            - name: duckduckgo
              engine: duckduckgo
              shortcut: ddg
              timeout: 3.0
              disabled: false
              
          enabled_plugins:
            - 'Hash plugin'
            - 'Search on category select'
            - 'Tracker URL remover'
          EOF

      - name: Start test services
        run: |
          echo "ðŸš€ Starting test services..."
          # Pull images first to avoid timeout issues
          docker compose -f docker-compose.test.yml pull
          
          # Start services
          docker compose -f docker-compose.test.yml up -d --wait --wait-timeout 90
          
          # Wait for services to be ready
          echo "â³ Waiting for services to be ready..."
          sleep 30
          
          # Check service health
          echo "ðŸ” Checking service health..."
          docker compose -f docker-compose.test.yml ps
          
          # Show logs if any service is not running
          if ! docker compose -f docker-compose.test.yml ps | grep -q "running"; then
            echo "âš ï¸ Some services may not be running properly. Showing logs:"
            docker compose -f docker-compose.test.yml logs --tail=50
          fi

      - name: Verify service connectivity
        run: |
          echo "ðŸ”— Testing service connectivity..."
          
          # Test Qdrant with retries
          echo "ðŸ” Checking Qdrant health..."
          for i in {1..10}; do
            if curl -f http://localhost:6333/readyz 2>/dev/null; then
              echo "âœ… Qdrant is ready"
              curl -s -X GET "http://localhost:6333/collections" || true
              break
            fi
            echo "â³ Qdrant not ready, attempt $i/10"
            sleep 5
          done
          
          # Test SearXNG (with retries)
          echo "ðŸ” Checking SearXNG health..."
          for i in {1..5}; do
            if curl -f http://localhost:8081/healthz 2>/dev/null; then
              echo "âœ… SearXNG is ready"
              break
            fi
            echo "â³ SearXNG not ready, attempt $i/5"
            sleep 10
          done
          
          # Test Neo4j
          echo "ðŸ” Checking Neo4j health..."
          docker exec neo4j_test cypher-shell -u neo4j -p testpassword123 "RETURN 1" || echo "âš ï¸ Neo4j not ready"
          
          # Show final service status
          echo ""
          echo "ðŸ“‹ Final service status:"
          docker compose -f docker-compose.test.yml ps

      - name: Run integration tests
        env:
          # Core test environment configuration
          OPENAI_API_KEY: test-key-for-mocks
          VECTOR_DATABASE: qdrant
          DATABASE_PROVIDER: qdrant
          QDRANT_URL: http://localhost:6333
          QDRANT_API_KEY: ""
          QDRANT_COLLECTION_NAME: test_crawled_pages
          QDRANT_EMBEDDING_MODEL: text-embedding-3-small
          SEARXNG_URL: http://localhost:8081
          SEARXNG_TEST_URL: http://localhost:8081
          NEO4J_URI: bolt://localhost:7687
          NEO4J_USERNAME: neo4j
          NEO4J_PASSWORD: testpassword123
          USE_RERANKING: "false"
          RERANKER_MODEL: BAAI/bge-reranker-v2-m3
          CHUNK_SIZE: "2000"
          CHUNK_OVERLAP: "200"
          TEST_TIMEOUT: "30"
          TEST_PARALLEL_WORKERS: "4"
          PYTEST_VERBOSE: "true"
          PYTHONPATH: ${{ github.workspace }}/src
          TESTING: "true"
          CI: "true"
        run: |
          echo "ðŸ§ª Running integration tests..."
          uv run pytest tests/ -v --tb=short \
            --cov=src --cov-report=xml --cov-report=term-missing \
            -m "integration" \
            --maxfail=5 \
            --timeout=60 \
            --perf-monitor --perf-output=performance_metrics_integration.json

      - name: Show service logs on failure
        if: failure()
        run: |
          echo "ðŸ“‹ Service logs:"
          docker compose -f docker-compose.test.yml logs --tail=50

      - name: Stop test services
        if: always()
        run: |
          echo "ðŸ›‘ Stopping test services..."
          docker compose -f docker-compose.test.yml down -v

      - name: Upload integration test coverage artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: coverage-integration
          path: |
            coverage.xml
            coverage.json
            .coverage
          retention-days: 7
      
      - name: Upload integration performance metrics
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-metrics-integration
          path: performance_metrics_integration.json
          retention-days: 30

  # ========================================
  # COMBINED COVERAGE REPORT
  # ========================================
  coverage-report:
    name: Coverage Report & Status
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests]
    if: always()
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install UV
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          echo "$HOME/.cargo/bin" >> $GITHUB_PATH

      - name: Install dependencies
        run: uv sync --frozen

      - name: Download all coverage reports
        uses: actions/download-artifact@v4
        continue-on-error: true

      - name: Generate final coverage report
        run: |
          echo "ðŸ“Š Generating final coverage report..."
          if [ -f coverage.xml ]; then
            uv run coverage report --fail-under=${{ env.COVERAGE_THRESHOLD }} || echo "Coverage below threshold"
            uv run coverage html
          else
            echo "No coverage file found"
          fi

      - name: Archive coverage reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: coverage-reports-final
          path: |
            coverage.xml
            htmlcov/
          if-no-files-found: warn
          retention-days: 7

      - name: Add coverage status badge
        if: github.event_name == 'pull_request'
        run: |
          echo "ðŸ“ˆ Coverage status updated for PR #${{ github.event.number }}"

  # ========================================
  # SECURITY SCANNING
  # ========================================
  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest
    needs: lint
    if: github.event_name != 'pull_request' || github.event.pull_request.head.repo.full_name == github.repository
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'

      - name: Upload Trivy scan results to GitHub Security tab
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: 'trivy-results.sarif'

  # ========================================
  # PERFORMANCE METRICS AGGREGATION
  # ========================================
  performance-report:
    name: Performance Metrics Report
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests]
    if: always()
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Download all performance metrics
        uses: actions/download-artifact@v4
        with:
          pattern: performance-metrics-*
          path: performance-data
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Aggregate performance metrics
        run: |
          echo "ðŸ“Š Aggregating performance metrics..."
          python - << 'EOF'
          import json
          import os
          from pathlib import Path
          from datetime import datetime
          
          # Find all performance metric files
          perf_dir = Path("performance-data")
          all_metrics = {}
          
          if perf_dir.exists():
              for artifact_dir in perf_dir.iterdir():
                  if artifact_dir.is_dir():
                      for json_file in artifact_dir.glob("*.json"):
                          with open(json_file) as f:
                              data = json.load(f)
                              # Extract test group and Python version from filename
                              name = json_file.stem
                              all_metrics[name] = data
          
          # Create aggregated report
          report = {
              "timestamp": datetime.utcnow().isoformat() + "Z",
              "test_runs": all_metrics,
              "summary": {
                  "total_test_runs": len(all_metrics),
                  "total_tests": sum(run.get("session", {}).get("total_tests", 0) for run in all_metrics.values()),
                  "total_duration": sum(run.get("session", {}).get("duration", 0) for run in all_metrics.values())
              }
          }
          
          # Save aggregated report
          with open("aggregated_performance_metrics.json", "w") as f:
              json.dump(report, f, indent=2)
          
          print(f"Total test runs: {report['summary']['total_test_runs']}")
          print(f"Total tests executed: {report['summary']['total_tests']}")
          print(f"Total duration: {report['summary']['total_duration']:.2f}s")
          EOF
      
      - name: Upload aggregated performance metrics
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: aggregated-performance-metrics
          path: aggregated_performance_metrics.json
          retention-days: 90

  # ========================================
  # BUILD VALIDATION
  # ========================================
  build-test:
    name: Build & Docker Test
    runs-on: ubuntu-latest
    needs: lint
    timeout-minutes: 15
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build Docker image
        run: |
          echo "ðŸ³ Building Docker image..."
          docker build --target development -t crawl4ai-mcp:test .

      - name: Test Docker image
        run: |
          echo "ðŸ§ª Testing Docker image..."
          docker run --rm crawl4ai-mcp:test python -c "import src.crawl4ai_mcp; print('MCP module loads successfully')"